{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25be9830",
   "metadata": {},
   "source": [
    "### Active Mini-Batch Stochastic Variational Inference\n",
    "\n",
    "Authors: Sushil Bohara, Dequan Yang, Bishnu Dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a813e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset, Subset, Dataset\n",
    "from torchvision import datasets, transforms\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro import poutine\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from pyro.optim import Adam\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import random\n",
    "from collections import Counter\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a94b915",
   "metadata": {},
   "source": [
    "##### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88dd3d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "SEED = 42\n",
    "pyro.set_rng_seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Data loading parameters\n",
    "BATCH_SIZE = 128\n",
    "TEST_BATCH_SIZE = 1000\n",
    "DATA_DIR = './data'\n",
    "\n",
    "# Model parameters\n",
    "INPUT_SIZE = 28 * 28  # Fashion-MNIST images are 28x28\n",
    "OUTPUT_SIZE = 10      # 10 clothing item classes\n",
    "\n",
    "# Training parameters\n",
    "NUM_EPOCHS = 5\n",
    "LEARNING_RATE = 0.01\n",
    "\n",
    "# Active learning parameters\n",
    "UNCERTAINTY_UPDATE_INTERVAL = 2  # Update uncertainty more frequently\n",
    "ACTIVE_BATCH_SIZE = 128          # Size of active minibatch\n",
    "WARM_UP_ITERATIONS = 200         # Longer warm-up with random sampling\n",
    "EXPLORATION_RATIO_START = 0.7    # Start with more exploration\n",
    "EXPLORATION_RATIO_END = 0.3      # End with more exploitation\n",
    "MAX_ITERATIONS_TO_REUSE = 100    # Prevent reusing same samples too frequently\n",
    "\n",
    "# Class imbalance parameters\n",
    "RARE_CLASSES = [7, 9]            # Sneaker and Ankle boot will be rare classes\n",
    "IMBALANCE_RATIO = 20             # Keep only 1/20 of the rare class samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3b8a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImbalancedDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Creates an imbalanced dataset from a base dataset\n",
    "    by undersampling specified rare classes.\n",
    "    \"\"\"\n",
    "    def __init__(self, base_dataset, rare_classes, imbalance_ratio):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            base_dataset: Original dataset\n",
    "            rare_classes: List of class indices to make rare\n",
    "            imbalance_ratio: Ratio to undersample rare classes (e.g., 10 means keep 1/10 of samples)\n",
    "        \"\"\"\n",
    "        self.base_dataset = base_dataset\n",
    "        self.rare_classes = rare_classes\n",
    "        self.imbalance_ratio = imbalance_ratio\n",
    "        \n",
    "        # Create an index mapping from our dataset to the base dataset\n",
    "        self.indices = self._create_imbalanced_indices()\n",
    "        \n",
    "        print(f\"Original dataset size: {len(base_dataset)}\")\n",
    "        print(f\"Imbalanced dataset size: {len(self.indices)}\")\n",
    "        \n",
    "        # Count class distribution\n",
    "        self.class_counts = Counter([base_dataset[i][1] for i in self.indices])\n",
    "        for class_idx in range(10):\n",
    "            count = self.class_counts[class_idx]\n",
    "            is_rare = \"RARE\" if class_idx in rare_classes else \"common\"\n",
    "            print(f\"Class {class_idx} ({is_rare}): {count} samples\")\n",
    "    \n",
    "    def _create_imbalanced_indices(self):\n",
    "        \"\"\"Create indices list with rare classes undersampled\"\"\"\n",
    "        indices = []\n",
    "        \n",
    "        # Group indices by class\n",
    "        class_indices = {i: [] for i in range(10)}\n",
    "        for i in range(len(self.base_dataset)):\n",
    "            _, label = self.base_dataset[i]\n",
    "            class_indices[label].append(i)\n",
    "        \n",
    "        # Add undersampled rare classes and all common classes\n",
    "        for class_idx, class_specific_indices in class_indices.items():\n",
    "            if class_idx in self.rare_classes:\n",
    "                # Undersample rare classes\n",
    "                random.shuffle(class_specific_indices)\n",
    "                num_to_keep = max(1, len(class_specific_indices) // self.imbalance_ratio)\n",
    "                indices.extend(class_specific_indices[:num_to_keep])\n",
    "            else:\n",
    "                # Keep all samples of common classes\n",
    "                indices.extend(class_specific_indices)\n",
    "                \n",
    "        return indices\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        base_idx = self.indices[idx]\n",
    "        return self.base_dataset[base_idx]\n",
    "\n",
    "\n",
    "class OptimizedActiveDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Optimized dataset wrapper for active learning\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "        self.uncertainties = torch.zeros(len(dataset))\n",
    "        self.sample_usage_counter = Counter()  # Track usage of samples\n",
    "        self.update_required = True\n",
    "        self.previously_selected = set()  # Track recently selected indices\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset[idx]\n",
    "    \n",
    "    def update_uncertainties(self, uncertainties):\n",
    "        \"\"\"Update the uncertainty values for all samples\"\"\"\n",
    "        assert len(uncertainties) == len(self.dataset), \"Uncertainties must match dataset size\"\n",
    "        self.uncertainties = uncertainties\n",
    "        self.update_required = False\n",
    "    \n",
    "    def get_stratified_batch_indices(self, batch_size):\n",
    "        \"\"\"\n",
    "        Return indices with stratified sampling based on uncertainty:\n",
    "        - Select from different uncertainty levels proportionally\n",
    "        - Ensures diversity across uncertainty spectrum\n",
    "        \"\"\"\n",
    "        if self.update_required:\n",
    "            # If update is required but not performed, return random indices\n",
    "            return torch.randperm(len(self.dataset))[:batch_size]\n",
    "        \n",
    "        # Divide samples into 5 uncertainty buckets (quintiles)\n",
    "        _, sorted_indices = torch.sort(self.uncertainties)\n",
    "        bucket_size = len(sorted_indices) // 5\n",
    "        buckets = [\n",
    "            sorted_indices[0:bucket_size],                    # Very certain\n",
    "            sorted_indices[bucket_size:2*bucket_size],        # Somewhat certain\n",
    "            sorted_indices[2*bucket_size:3*bucket_size],      # Medium certainty\n",
    "            sorted_indices[3*bucket_size:4*bucket_size],      # Somewhat uncertain\n",
    "            sorted_indices[4*bucket_size:],                   # Very uncertain\n",
    "        ]\n",
    "        \n",
    "        # Sample from each bucket proportionally\n",
    "        # More from higher uncertainty buckets, but some from all\n",
    "        proportions = [0.1, 0.15, 0.2, 0.25, 0.3]  # Distribution across buckets\n",
    "        \n",
    "        selected_indices = []\n",
    "        for i, bucket in enumerate(buckets):\n",
    "            # Calculate how many samples to take from this bucket\n",
    "            n_samples = int(batch_size * proportions[i])\n",
    "            \n",
    "            # Filter out recently used samples to reduce repetition\n",
    "            available_indices = set(bucket.tolist()) - self.previously_selected\n",
    "            if len(available_indices) < n_samples:  # Fall back if not enough\n",
    "                available_indices = set(bucket.tolist())\n",
    "                \n",
    "            # Convert back to list and get a random sample\n",
    "            available_indices = list(available_indices)\n",
    "            if len(available_indices) > 0:\n",
    "                bucket_indices = random.sample(\n",
    "                    available_indices, \n",
    "                    min(n_samples, len(available_indices))\n",
    "                )\n",
    "                selected_indices.extend(bucket_indices)\n",
    "                \n",
    "                # Add to recently used set\n",
    "                self.previously_selected.update(bucket_indices)\n",
    "        \n",
    "        # If we didn't get enough samples, fill with random ones\n",
    "        if len(selected_indices) < batch_size:\n",
    "            remaining = batch_size - len(selected_indices)\n",
    "            available_indices = set(range(len(self.dataset))) - set(selected_indices) - self.previously_selected\n",
    "            if len(available_indices) < remaining:\n",
    "                available_indices = set(range(len(self.dataset))) - set(selected_indices)\n",
    "            \n",
    "            available_indices = list(available_indices)\n",
    "            additional_indices = random.sample(\n",
    "                available_indices, \n",
    "                min(remaining, len(available_indices))\n",
    "            )\n",
    "            selected_indices.extend(additional_indices)\n",
    "            self.previously_selected.update(additional_indices)\n",
    "        \n",
    "        # Limit the size of previously_selected to prevent memory issues\n",
    "        if len(self.previously_selected) > MAX_ITERATIONS_TO_REUSE:\n",
    "            self.previously_selected = set(list(self.previously_selected)[-MAX_ITERATIONS_TO_REUSE:])\n",
    "            \n",
    "        # Ensure we have batch_size samples\n",
    "        if len(selected_indices) > batch_size:\n",
    "            selected_indices = selected_indices[:batch_size]\n",
    "        elif len(selected_indices) < batch_size:\n",
    "            # This should rarely happen, but just in case\n",
    "            remaining = batch_size - len(selected_indices)\n",
    "            selected_indices.extend(torch.randperm(len(self.dataset))[:remaining].tolist())\n",
    "            \n",
    "        # Track usage\n",
    "        for idx in selected_indices:\n",
    "            self.sample_usage_counter[idx] += 1\n",
    "            \n",
    "        return torch.tensor(selected_indices)\n",
    "    \n",
    "    def get_random_batch_indices(self, batch_size):\n",
    "        \"\"\"Return random indices for comparison\"\"\"\n",
    "        indices = torch.randperm(len(self.dataset))[:batch_size].tolist()\n",
    "        \n",
    "        # Track usage\n",
    "        for idx in indices:\n",
    "            self.sample_usage_counter[idx] += 1\n",
    "            \n",
    "        return torch.tensor(indices)\n",
    "\n",
    "\n",
    "class BayesianLogisticRegression(nn.Module):\n",
    "    \"\"\"\n",
    "    Bayesian Logistic Regression model for Fashion-MNIST classification.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(BayesianLogisticRegression, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Flatten the input image\n",
    "        x = x.view(-1, INPUT_SIZE)\n",
    "        # Apply linear transformation\n",
    "        return self.linear(x)\n",
    "\n",
    "\n",
    "def model(x_data, y_data=None):\n",
    "    \"\"\"\n",
    "    Bayesian logistic regression model.\n",
    "    \"\"\"\n",
    "    # Define priors\n",
    "    weight_prior = dist.Normal(0., 1.).expand([OUTPUT_SIZE, INPUT_SIZE]).to_event(2)\n",
    "    bias_prior = dist.Normal(0., 1.).expand([OUTPUT_SIZE]).to_event(1)\n",
    "    \n",
    "    # Sample from priors\n",
    "    priors = {}\n",
    "    priors['linear.weight'] = pyro.sample('linear.weight', weight_prior)\n",
    "    priors['linear.bias'] = pyro.sample('linear.bias', bias_prior)\n",
    "    \n",
    "    # Forward pass\n",
    "    x = x_data.view(-1, INPUT_SIZE)\n",
    "    logits = torch.matmul(x, priors['linear.weight'].t()) + priors['linear.bias']\n",
    "    \n",
    "    # Sample y from the logistic model\n",
    "    with pyro.plate('data', x.shape[0]):\n",
    "        obs = pyro.sample('obs', dist.Categorical(logits=logits), obs=y_data)\n",
    "        \n",
    "    return logits, obs\n",
    "\n",
    "\n",
    "def guide(x_data, y_data=None):\n",
    "    \"\"\"\n",
    "    Variational guide (posterior approximation) for the Bayesian logistic regression model.\n",
    "    \"\"\"\n",
    "    # Define variational parameters\n",
    "    # For the weight matrix\n",
    "    w_loc = pyro.param('w_loc', torch.zeros(OUTPUT_SIZE, INPUT_SIZE).to(device))\n",
    "    w_scale = pyro.param('w_scale', torch.ones(OUTPUT_SIZE, INPUT_SIZE).to(device),\n",
    "                          constraint=dist.constraints.positive)\n",
    "    \n",
    "    # For the bias vector\n",
    "    b_loc = pyro.param('b_loc', torch.zeros(OUTPUT_SIZE).to(device))\n",
    "    b_scale = pyro.param('b_scale', torch.ones(OUTPUT_SIZE).to(device),\n",
    "                         constraint=dist.constraints.positive)\n",
    "    \n",
    "    # Sample from variational distributions\n",
    "    w = pyro.sample('linear.weight', dist.Normal(w_loc, w_scale).to_event(2))\n",
    "    b = pyro.sample('linear.bias', dist.Normal(b_loc, b_scale).to_event(1))\n",
    "\n",
    "\n",
    "def compute_bayesian_uncertainty(model, guide, x_data, num_samples=10, uncertainty_type='entropy'):\n",
    "    \"\"\"\n",
    "    Compute uncertainty measures for each sample.\n",
    "    \n",
    "    Args:\n",
    "        model: Bayesian model\n",
    "        guide: Variational guide\n",
    "        x_data: Input data tensor\n",
    "        num_samples: Number of posterior samples to use\n",
    "        uncertainty_type: Type of uncertainty measure to use\n",
    "                         'entropy': predictive entropy\n",
    "                         'bald': Bayesian Active Learning by Disagreement\n",
    "                         'variation_ratio': 1 - max probability\n",
    "    \"\"\"\n",
    "    all_probs = []\n",
    "    \n",
    "    # Obtain samples from the posterior\n",
    "    for _ in range(num_samples):\n",
    "        # Sample parameters from the guide\n",
    "        guide_trace = poutine.trace(guide).get_trace(x_data)\n",
    "        sampled_weights = guide_trace.nodes['linear.weight']['value']\n",
    "        sampled_bias = guide_trace.nodes['linear.bias']['value']\n",
    "        \n",
    "        # Forward pass with sampled parameters\n",
    "        with torch.no_grad():\n",
    "            x = x_data.view(-1, INPUT_SIZE)\n",
    "            logits = torch.matmul(x, sampled_weights.t()) + sampled_bias\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            all_probs.append(probs)\n",
    "    \n",
    "    # Stack probabilities from all samples [num_samples, batch_size, num_classes]\n",
    "    stacked_probs = torch.stack(all_probs)\n",
    "    \n",
    "    # Average probabilities across samples [batch_size, num_classes]\n",
    "    mean_probs = stacked_probs.mean(0)\n",
    "    \n",
    "    if uncertainty_type == 'entropy':\n",
    "        # Predictive entropy: -∑p*log(p)\n",
    "        entropy = -torch.sum(mean_probs * torch.log(mean_probs + 1e-10), dim=1)\n",
    "        return entropy\n",
    "        \n",
    "    elif uncertainty_type == 'bald':\n",
    "        # Bayesian Active Learning by Disagreement\n",
    "        # BALD = H(y|x) - E_θ[H(y|x,θ)]\n",
    "        \n",
    "        # First term: entropy of the mean prediction (same as predictive entropy)\n",
    "        H_mean = -torch.sum(mean_probs * torch.log(mean_probs + 1e-10), dim=1)\n",
    "        \n",
    "        # Second term: mean entropy of individual predictions\n",
    "        sample_entropies = -torch.sum(stacked_probs * torch.log(stacked_probs + 1e-10), dim=2)\n",
    "        mean_entropy = sample_entropies.mean(0)\n",
    "        \n",
    "        # BALD score\n",
    "        bald = H_mean - mean_entropy\n",
    "        return bald\n",
    "        \n",
    "    elif uncertainty_type == 'variation_ratio':\n",
    "        # Variation ratio = 1 - max probability\n",
    "        max_probs, _ = torch.max(mean_probs, dim=1)\n",
    "        variation_ratio = 1.0 - max_probs\n",
    "        return variation_ratio\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown uncertainty type: {uncertainty_type}\")\n",
    "\n",
    "\n",
    "def predict(model, guide, x, num_samples=10):\n",
    "    \"\"\"\n",
    "    Make predictions with the trained model.\n",
    "    \"\"\"\n",
    "    # Obtain samples from the posterior\n",
    "    all_probs = []\n",
    "    for _ in range(num_samples):\n",
    "        # Sample parameters from the guide\n",
    "        guide_trace = poutine.trace(guide).get_trace(x)\n",
    "        sampled_weights = guide_trace.nodes['linear.weight']['value']\n",
    "        sampled_bias = guide_trace.nodes['linear.bias']['value']\n",
    "        \n",
    "        # Forward pass with sampled parameters\n",
    "        with torch.no_grad():\n",
    "            x_flat = x.view(-1, INPUT_SIZE)\n",
    "            logits = torch.matmul(x_flat, sampled_weights.t()) + sampled_bias\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            all_probs.append(probs)\n",
    "    \n",
    "    # Average probabilities across samples\n",
    "    mean_probs = torch.stack(all_probs).mean(0)\n",
    "    _, predicted_class = torch.max(mean_probs, 1)\n",
    "    \n",
    "    return predicted_class, mean_probs\n",
    "\n",
    "\n",
    "def evaluate_per_class(model, guide, test_loader, class_names):\n",
    "    \"\"\"\n",
    "    Evaluate the model on test data and report per-class accuracy.\n",
    "    \"\"\"\n",
    "    class_correct = {i: 0 for i in range(OUTPUT_SIZE)}\n",
    "    class_total = {i: 0 for i in range(OUTPUT_SIZE)}\n",
    "    overall_correct = 0\n",
    "    overall_total = 0\n",
    "    uncertainties = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            predicted_class, probs = predict(model, guide, data)\n",
    "            \n",
    "            # Calculate prediction entropy as uncertainty measure\n",
    "            entropy = -torch.sum(probs * torch.log(probs + 1e-10), dim=1)\n",
    "            uncertainties.extend(entropy.cpu().numpy())\n",
    "            \n",
    "            # Track per-class accuracy\n",
    "            for i in range(len(target)):\n",
    "                label = target[i].item()\n",
    "                pred = predicted_class[i].item()\n",
    "                if label == pred:\n",
    "                    class_correct[label] += 1\n",
    "                    overall_correct += 1\n",
    "                class_total[label] += 1\n",
    "                overall_total += 1\n",
    "    \n",
    "    # Print overall accuracy\n",
    "    overall_accuracy = 100.0 * overall_correct / overall_total\n",
    "    avg_uncertainty = np.mean(uncertainties)\n",
    "    \n",
    "    print(f'Overall Test Accuracy: {overall_accuracy:.2f}%')\n",
    "    print(f'Average Uncertainty: {avg_uncertainty:.4f}')\n",
    "    \n",
    "    # Print per-class accuracy\n",
    "    print(\"\\nPer-class accuracy:\")\n",
    "    for i in range(OUTPUT_SIZE):\n",
    "        if class_total[i] > 0:\n",
    "            accuracy = 100.0 * class_correct[i] / class_total[i]\n",
    "            print(f'  Class {i} ({class_names[i]}): {accuracy:.2f}% ({class_correct[i]}/{class_total[i]})')\n",
    "        else:\n",
    "            print(f'  Class {i} ({class_names[i]}): No test samples')\n",
    "    \n",
    "    # Rare class performance is especially important\n",
    "    rare_class_correct = sum(class_correct[c] for c in RARE_CLASSES)\n",
    "    rare_class_total = sum(class_total[c] for c in RARE_CLASSES)\n",
    "    if rare_class_total > 0:\n",
    "        rare_class_accuracy = 100.0 * rare_class_correct / rare_class_total\n",
    "        print(f'\\nRare class accuracy: {rare_class_accuracy:.2f}% ({rare_class_correct}/{rare_class_total})')\n",
    "    \n",
    "    return overall_accuracy, rare_class_accuracy, uncertainties\n",
    "\n",
    "\n",
    "def train_with_optimized_active_minibatches(model, guide, active_dataset, optimizer, num_epochs, \n",
    "                                 batch_size=BATCH_SIZE, update_interval=UNCERTAINTY_UPDATE_INTERVAL,\n",
    "                                 uncertainty_type='bald', class_names=None):\n",
    "    \"\"\"\n",
    "    Train the model using SVI with optimized active minibatch selection.\n",
    "    \"\"\"\n",
    "    # Define SVI\n",
    "    svi = SVI(model, guide, optimizer, loss=Trace_ELBO())\n",
    "    \n",
    "    # Training loop\n",
    "    train_losses = []\n",
    "    test_overall_accuracies = []\n",
    "    test_rare_accuracies = []\n",
    "    \n",
    "    # Get all data to compute uncertainties when needed\n",
    "    all_data = torch.stack([active_dataset[i][0] for i in range(len(active_dataset))]).to(device)\n",
    "    all_labels = torch.tensor([active_dataset[i][1] for i in range(len(active_dataset))]).to(device)\n",
    "    \n",
    "    # Create test loader for evaluation\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.2860,), (0.3530,))  # Fashion-MNIST mean and std\n",
    "    ])\n",
    "    test_dataset = datasets.FashionMNIST(DATA_DIR, train=False, transform=transform)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=TEST_BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    # Evaluate model before training begins to establish a baseline\n",
    "    print(\"\\nEvaluating baseline model performance...\")\n",
    "    initial_accuracy, initial_rare_accuracy, _ = evaluate_per_class(model, guide, test_loader, class_names)\n",
    "    test_overall_accuracies.append((0, initial_accuracy))\n",
    "    test_rare_accuracies.append((0, initial_rare_accuracy))\n",
    "    \n",
    "    total_iterations = num_epochs * (len(active_dataset) // batch_size)\n",
    "    \n",
    "    # Training iterations\n",
    "    iteration = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        # Initialize loss for this epoch\n",
    "        epoch_loss = 0.0\n",
    "        processed_batches = 0\n",
    "        \n",
    "        # Number of batches per epoch\n",
    "        num_batches = len(active_dataset) // batch_size\n",
    "        \n",
    "        for batch_idx in tqdm(range(num_batches), desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            # Initial warm-up phase with random batches\n",
    "            if iteration < WARM_UP_ITERATIONS:\n",
    "                batch_indices = active_dataset.get_random_batch_indices(batch_size)\n",
    "            else:\n",
    "                # Update uncertainties periodically\n",
    "                if iteration % update_interval == 0 or active_dataset.update_required:\n",
    "                    print(\"\\nUpdating uncertainties...\")\n",
    "                    # Compute uncertainties for all samples using the specified method\n",
    "                    uncertainties = compute_bayesian_uncertainty(\n",
    "                        model, guide, all_data, \n",
    "                        num_samples=10, \n",
    "                        uncertainty_type=uncertainty_type\n",
    "                    )\n",
    "                    active_dataset.update_uncertainties(uncertainties.cpu())\n",
    "                \n",
    "                # Calculate adaptive exploration ratio based on training progress\n",
    "                # Start with more exploration and gradually focus more on exploitation\n",
    "                progress = min(1.0, (iteration - WARM_UP_ITERATIONS) / (total_iterations - WARM_UP_ITERATIONS))\n",
    "                exploration_ratio = EXPLORATION_RATIO_START + progress * (EXPLORATION_RATIO_END - EXPLORATION_RATIO_START)\n",
    "                \n",
    "                # Get stratified batch indices\n",
    "                batch_indices = active_dataset.get_stratified_batch_indices(batch_size)\n",
    "            \n",
    "            # Get batch data\n",
    "            batch_data = all_data[batch_indices]\n",
    "            batch_labels = all_labels[batch_indices]\n",
    "            \n",
    "            # Compute loss on this batch\n",
    "            batch_loss = svi.step(batch_data, batch_labels)\n",
    "            epoch_loss += batch_loss\n",
    "            processed_batches += 1\n",
    "            \n",
    "            # Print intermediary results\n",
    "            if batch_idx % 20 == 0:\n",
    "                print(f'Batch {batch_idx}/{num_batches}, Loss: {batch_loss / batch_size:.6f}')\n",
    "            \n",
    "            # Track metrics consistently, including during warm-up phase\n",
    "            if iteration % update_interval == 0:\n",
    "                accuracy, rare_accuracy, _ = evaluate_per_class(model, guide, test_loader, class_names)\n",
    "                test_overall_accuracies.append((iteration, accuracy))\n",
    "                test_rare_accuracies.append((iteration, rare_accuracy))\n",
    "            \n",
    "            iteration += 1\n",
    "        \n",
    "        # Calculate average loss over the epoch\n",
    "        avg_epoch_loss = epoch_loss / (processed_batches * batch_size)\n",
    "        train_losses.append(avg_epoch_loss)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_epoch_loss:.6f}')\n",
    "        \n",
    "        # Evaluate model after each epoch\n",
    "        accuracy, rare_accuracy, _ = evaluate_per_class(model, guide, test_loader, class_names)\n",
    "        test_overall_accuracies.append((iteration, accuracy))\n",
    "        test_rare_accuracies.append((iteration, rare_accuracy))\n",
    "    \n",
    "    return train_losses, test_overall_accuracies, test_rare_accuracies\n",
    "\n",
    "\n",
    "def train_with_random_minibatches(model, guide, active_dataset, optimizer, num_epochs, \n",
    "                                  batch_size=BATCH_SIZE, class_names=None):\n",
    "    \"\"\"\n",
    "    Train the model using SVI with random minibatch selection (baseline).\n",
    "    \"\"\"\n",
    "    # Define SVI\n",
    "    svi = SVI(model, guide, optimizer, loss=Trace_ELBO())\n",
    "    \n",
    "    # Training loop\n",
    "    train_losses = []\n",
    "    test_overall_accuracies = []\n",
    "    test_rare_accuracies = []\n",
    "    \n",
    "    # Get all data \n",
    "    all_data = torch.stack([active_dataset[i][0] for i in range(len(active_dataset))]).to(device)\n",
    "    all_labels = torch.tensor([active_dataset[i][1] for i in range(len(active_dataset))]).to(device)\n",
    "    \n",
    "    # Create test loader for evaluation\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.2860,), (0.3530,))  # Fashion-MNIST mean and std\n",
    "    ])\n",
    "    test_dataset = datasets.FashionMNIST(DATA_DIR, train=False, transform=transform)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=TEST_BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    total_iterations = num_epochs * (len(active_dataset) // batch_size)\n",
    "    \n",
    "    # Training iterations\n",
    "    iteration = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        # Initialize loss for this epoch\n",
    "        epoch_loss = 0.0\n",
    "        processed_batches = 0\n",
    "        \n",
    "        # Number of batches per epoch\n",
    "        num_batches = len(active_dataset) // batch_size\n",
    "        \n",
    "        for batch_idx in tqdm(range(num_batches), desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            # Get random minibatch indices\n",
    "            batch_indices = active_dataset.get_random_batch_indices(batch_size)\n",
    "            \n",
    "            # Get batch data\n",
    "            batch_data = all_data[batch_indices]\n",
    "            batch_labels = all_labels[batch_indices]\n",
    "            \n",
    "            # Compute loss on this batch\n",
    "            batch_loss = svi.step(batch_data, batch_labels)\n",
    "            epoch_loss += batch_loss\n",
    "            processed_batches += 1\n",
    "            \n",
    "            # Print intermediary results\n",
    "            if batch_idx % 20 == 0:\n",
    "                print(f'Batch {batch_idx}/{num_batches}, Loss: {batch_loss / batch_size:.6f}')\n",
    "            \n",
    "            # For monitoring - compute test accuracy periodically\n",
    "            if iteration % UNCERTAINTY_UPDATE_INTERVAL == 0 and iteration > 0:\n",
    "                accuracy, rare_accuracy, _ = evaluate_per_class(model, guide, test_loader, class_names)\n",
    "                test_overall_accuracies.append((iteration, accuracy))\n",
    "                test_rare_accuracies.append((iteration, rare_accuracy))\n",
    "            \n",
    "            iteration += 1\n",
    "        \n",
    "        # Calculate average loss over the epoch\n",
    "        avg_epoch_loss = epoch_loss / (processed_batches * batch_size)\n",
    "        train_losses.append(avg_epoch_loss)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_epoch_loss:.6f}')\n",
    "        \n",
    "        # Evaluate model after each epoch\n",
    "        accuracy, rare_accuracy, _ = evaluate_per_class(model, guide, test_loader, class_names)\n",
    "        test_overall_accuracies.append((iteration, accuracy))\n",
    "        test_rare_accuracies.append((iteration, rare_accuracy))\n",
    "        \n",
    "    return train_losses, test_overall_accuracies, test_rare_accuracies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bd6867",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_comparison(active_losses, random_losses, \n",
    "                   active_overall_accuracies, random_overall_accuracies,\n",
    "                   active_rare_accuracies, random_rare_accuracies):\n",
    "    \"\"\"\n",
    "    Plot comparison between active and random training methods with cumulative averages.\n",
    "    \n",
    "    Args:\n",
    "        active_losses: List of losses from active minibatch training\n",
    "        random_losses: List of losses from random minibatch training\n",
    "        active_overall_accuracies: List of (iteration, accuracy) tuples from active minibatch\n",
    "        random_overall_accuracies: List of (iteration, accuracy) tuples from random minibatch\n",
    "        active_rare_accuracies: List of (iteration, accuracy) tuples for rare classes from active\n",
    "        random_rare_accuracies: List of (iteration, accuracy) tuples for rare classes from random\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(15, 20))\n",
    "    \n",
    "    # Plot losses\n",
    "    plt.subplot(4, 1, 1)\n",
    "    plt.plot(active_losses, 'b-', label='Optimized Active Minibatch')\n",
    "    plt.plot(random_losses, 'r-', label='Random Minibatch')\n",
    "    plt.xlabel('Epoch', fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('ELBO Loss', fontsize=14, fontweight='bold')\n",
    "    plt.title('Training Loss Comparison', fontsize=16, fontweight='bold')\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.grid(True)\n",
    "    plt.tick_params(axis='both', which='major', labelsize=12)\n",
    "    \n",
    "    # Extract iteration and accuracy values\n",
    "    active_iters = np.array([acc[0] for acc in active_overall_accuracies])\n",
    "    active_acc = np.array([acc[1] for acc in active_overall_accuracies])\n",
    "    \n",
    "    random_iters = np.array([acc[0] for acc in random_overall_accuracies])\n",
    "    random_acc = np.array([acc[1] for acc in random_overall_accuracies])\n",
    "    \n",
    "    # Extract iteration and accuracy values for rare classes\n",
    "    active_rare_iters = np.array([acc[0] for acc in active_rare_accuracies])\n",
    "    active_rare_acc = np.array([acc[1] for acc in active_rare_accuracies])\n",
    "    \n",
    "    random_rare_iters = np.array([acc[0] for acc in random_rare_accuracies])\n",
    "    random_rare_acc = np.array([acc[1] for acc in random_rare_accuracies])\n",
    "    \n",
    "    # Plot raw overall accuracies\n",
    "    plt.subplot(4, 1, 2)\n",
    "    plt.plot(active_iters, active_acc, 'b-', alpha=0.5, label='Active (Raw)')\n",
    "    plt.plot(random_iters, random_acc, 'r-', alpha=0.5, label='Random (Raw)')\n",
    "    plt.xlabel('Iteration', fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('Overall Test Accuracy (%)', fontsize=14, fontweight='bold')\n",
    "    plt.title('Overall Test Accuracy Comparison', fontsize=16, fontweight='bold')\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.grid(True)\n",
    "    plt.tick_params(axis='both', which='major', labelsize=12)\n",
    "    \n",
    "    # Plot raw rare class accuracies\n",
    "    plt.subplot(4, 1, 3)\n",
    "    plt.plot(active_rare_iters, active_rare_acc, 'b-', alpha=0.5, label='Active (Raw)')\n",
    "    plt.plot(random_rare_iters, random_rare_acc, 'r-', alpha=0.5, label='Random (Raw)')\n",
    "    plt.xlabel('Iteration', fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('Rare Class Test Accuracy (%)', fontsize=14, fontweight='bold')\n",
    "    plt.title('Rare Class Test Accuracy Comparison (Sneakers & Ankle Boots)', fontsize=16, fontweight='bold')\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.grid(True)\n",
    "    plt.tick_params(axis='both', which='major', labelsize=12)\n",
    "    \n",
    "    # Plot relative improvement: only cumulative average improvements\n",
    "    plt.subplot(4, 1, 4)\n",
    "    \n",
    "    def interpolate_at_iterations(source_iters, source_values, target_iters):\n",
    "        from scipy.interpolate import interp1d\n",
    "        if len(source_iters) < 2:\n",
    "            return np.zeros_like(target_iters)\n",
    "        sort_idx = np.argsort(source_iters)\n",
    "        sorted_iters = source_iters[sort_idx]\n",
    "        sorted_values = source_values[sort_idx]\n",
    "        interp_func = interp1d(sorted_iters, sorted_values, bounds_error=False, fill_value=\"extrapolate\")\n",
    "        return interp_func(target_iters)\n",
    "    \n",
    "    min_iter = max(min(active_iters), min(random_iters))\n",
    "    max_iter = min(max(active_iters), max(random_iters))\n",
    "    comparison_iters = np.linspace(min_iter, max_iter, 100)\n",
    "    \n",
    "    active_interp = interpolate_at_iterations(active_iters, active_acc, comparison_iters)\n",
    "    random_interp = interpolate_at_iterations(random_iters, random_acc, comparison_iters)\n",
    "    overall_improvement = active_interp - random_interp\n",
    "    \n",
    "    active_rare_interp = interpolate_at_iterations(active_rare_iters, active_rare_acc, comparison_iters)\n",
    "    random_rare_interp = interpolate_at_iterations(random_rare_iters, random_rare_acc, comparison_iters)\n",
    "    rare_improvement = active_rare_interp - random_rare_interp\n",
    "    \n",
    "    # Cumulative averages only\n",
    "    cumulative_avg_overall = np.cumsum(overall_improvement) / (np.arange(len(overall_improvement)) + 1)\n",
    "    cumulative_avg_rare = np.cumsum(rare_improvement) / (np.arange(len(rare_improvement)) + 1)\n",
    "    \n",
    "    # Print the final cumulative average improvements\n",
    "    print(f\"Final Cumulative Average Overall Improvement: {cumulative_avg_overall[-1]:.2f}%\")\n",
    "    print(f\"Final Cumulative Average Rare Class Improvement: {cumulative_avg_rare[-1]:.2f}%\")\n",
    "    \n",
    "    plt.plot(comparison_iters, cumulative_avg_overall, 'g-', linewidth=2, \n",
    "             label='Cumulative Avg Overall Improvement')\n",
    "    plt.plot(comparison_iters, cumulative_avg_rare, 'm-', linewidth=2, \n",
    "             label='Cumulative Avg Rare Class Improvement')\n",
    "    \n",
    "    plt.axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "    plt.xlabel('Iteration', fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('Average Improvement (Active - Random) (%)', fontsize=13, fontweight='bold')\n",
    "    plt.title('Average Improvement of Active Learning over Random Sampling', fontsize=14, fontweight='bold')\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.grid(True)\n",
    "    plt.tick_params(axis='both', which='major', labelsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('imbalanced_comparison.png', dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7f8688",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_dataset_usage(active_dataset, class_names):\n",
    "    \"\"\"\n",
    "    Analyze how the dataset was used during training.\n",
    "    \"\"\"\n",
    "    # Get sample usage statistics\n",
    "    usage_counts = list(active_dataset.sample_usage_counter.values())\n",
    "    \n",
    "    if not usage_counts:\n",
    "        print(\"No usage data available\")\n",
    "        return\n",
    "    \n",
    "    # Calculate statistics\n",
    "    min_usage = min(usage_counts)\n",
    "    max_usage = max(usage_counts)\n",
    "    mean_usage = sum(usage_counts) / len(usage_counts)\n",
    "    \n",
    "    # Calculate how many samples were never used\n",
    "    never_used = len([count for count in usage_counts if count == 0])\n",
    "    \n",
    "    # Calculate histogram data\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(usage_counts, bins=20, alpha=0.7)\n",
    "    plt.axvline(mean_usage, color='r', linestyle='dashed', linewidth=1, label=f'Mean: {mean_usage:.2f}')\n",
    "    plt.xlabel('Number of Times Selected')\n",
    "    plt.ylabel('Number of Samples')\n",
    "    plt.title('Sample Usage Distribution')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    print(f\"Sample Usage Statistics:\")\n",
    "    print(f\"  - Min usage: {min_usage}\")\n",
    "    print(f\"  - Max usage: {max_usage}\")\n",
    "    print(f\"  - Mean usage: {mean_usage:.2f}\")\n",
    "    print(f\"  - Never used: {never_used} samples ({never_used/len(usage_counts)*100:.2f}%)\")\n",
    "    \n",
    "    # Analyze usage by class\n",
    "    class_labels = [active_dataset.dataset[i][1] for i in range(len(active_dataset.dataset))]\n",
    "    usage_by_class = {i: [] for i in range(OUTPUT_SIZE)}\n",
    "    \n",
    "    for i, (count, label) in enumerate(zip(usage_counts, class_labels)):\n",
    "        usage_by_class[label].append(count)\n",
    "    \n",
    "    # Print statistics by class\n",
    "    print(\"\\nUsage statistics by class:\")\n",
    "    for class_idx in range(OUTPUT_SIZE):\n",
    "        if usage_by_class[class_idx]:\n",
    "            class_mean = sum(usage_by_class[class_idx]) / len(usage_by_class[class_idx])\n",
    "            class_max = max(usage_by_class[class_idx]) if usage_by_class[class_idx] else 0\n",
    "            class_never = len([c for c in usage_by_class[class_idx] if c == 0])\n",
    "            \n",
    "            rare_label = \"(RARE)\" if class_idx in RARE_CLASSES else \"\"\n",
    "            print(f\"  - Class {class_idx} ({class_names[class_idx]}) {rare_label}:\")\n",
    "            print(f\"      Mean usage: {class_mean:.2f}\")\n",
    "            print(f\"      Max usage: {class_max}\")\n",
    "            print(f\"      Never used: {class_never} samples ({class_never/len(usage_by_class[class_idx])*100:.2f}%)\")\n",
    "    \n",
    "    plt.savefig('sample_usage_distribution.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1ecbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_uncertain_samples(active_dataset, k=10, class_names=None):\n",
    "    \"\"\"\n",
    "    Visualize the most uncertain samples.\n",
    "    \"\"\"\n",
    "    # Get indices of most uncertain samples\n",
    "    _, indices = torch.sort(active_dataset.uncertainties, descending=True)\n",
    "    top_k_indices = indices[:k].cpu().numpy()\n",
    "    \n",
    "    # Get data and labels\n",
    "    samples = [active_dataset[idx][0].cpu().numpy().reshape(28, 28) for idx in top_k_indices]\n",
    "    labels = [active_dataset[idx][1] for idx in top_k_indices]\n",
    "    uncertainty_values = [active_dataset.uncertainties[idx].item() for idx in top_k_indices]\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    for i in range(k):\n",
    "        plt.subplot(2, 5, i+1)\n",
    "        plt.imshow(samples[i], cmap='gray')\n",
    "        class_label = f\"{labels[i]} ({class_names[labels[i]]})\" if class_names else str(labels[i])\n",
    "        rare_label = \"(RARE)\" if labels[i] in RARE_CLASSES else \"\"\n",
    "        plt.title(f\"Label: {class_label}\\n{rare_label}\\nUncertainty: {uncertainty_values[i]:.4f}\")\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('most_uncertain_samples.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab2fc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_class_distribution(dataset, class_names):\n",
    "    \"\"\"\n",
    "    Visualize the class distribution in the dataset.\n",
    "    \"\"\"\n",
    "    # Count classes\n",
    "    class_counts = Counter([dataset[i][1] for i in range(len(dataset))])\n",
    "    \n",
    "    # Sort by class index\n",
    "    classes = sorted(class_counts.keys())\n",
    "    counts = [class_counts[cls] for cls in classes]\n",
    "    \n",
    "    # Create class labels with names\n",
    "    labels = [f\"{i} ({class_names[i]})\" for i in classes]\n",
    "    \n",
    "    # Highlight rare classes\n",
    "    colors = ['lightblue' if i not in RARE_CLASSES else 'red' for i in classes]\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    bars = plt.bar(labels, counts, color=colors)\n",
    "    plt.xlabel('Classes')\n",
    "    plt.ylabel('Number of Samples')\n",
    "    plt.title('Class Distribution in Imbalanced Dataset')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    \n",
    "    # Add a legend\n",
    "    from matplotlib.patches import Patch\n",
    "    legend_elements = [\n",
    "        Patch(facecolor='lightblue', label='Common Classes'),\n",
    "        Patch(facecolor='red', label='Rare Classes')\n",
    "    ]\n",
    "    plt.legend(handles=legend_elements)\n",
    "    \n",
    "    # Add counts on top of the bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + 5,\n",
    "                 f'{height}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('class_distribution.png')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3f79c0",
   "metadata": {},
   "source": [
    "### Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4d2a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data transformations \n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.2860,), (0.3530,))  # Fashion-MNIST mean and std\n",
    "])\n",
    "\n",
    "# Define Fashion-MNIST class names\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "                'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "# Load Fashion-MNIST dataset\n",
    "base_dataset = datasets.FashionMNIST(DATA_DIR, train=True, download=True, transform=transform)\n",
    "\n",
    "# Create imbalanced dataset\n",
    "imbalanced_dataset = ImbalancedDataset(base_dataset, RARE_CLASSES, IMBALANCE_RATIO)\n",
    "\n",
    "# Create active dataset wrapper\n",
    "active_dataset = OptimizedActiveDataset(imbalanced_dataset)\n",
    "\n",
    "# Visualize class distribution\n",
    "plot_class_distribution(imbalanced_dataset, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b065eb8d",
   "metadata": {},
   "source": [
    "##### Running Optimized Active Minibatch Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5f8d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For optimized active minibatch selection\n",
    "pyro.clear_param_store()\n",
    "active_optimizer = Adam({\"lr\": LEARNING_RATE})\n",
    "active_losses, active_overall_accuracies, active_rare_accuracies = train_with_optimized_active_minibatches(\n",
    "    model, guide, active_dataset, active_optimizer, NUM_EPOCHS, \n",
    "    batch_size=ACTIVE_BATCH_SIZE, update_interval=UNCERTAINTY_UPDATE_INTERVAL,\n",
    "    uncertainty_type='bald',  # Use BALD measure for better performance\n",
    "    class_names=class_names\n",
    ")\n",
    "\n",
    "# Save active model\n",
    "active_model_params = {name: param.data.clone() for name, param in pyro.get_param_store().items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d1819b",
   "metadata": {},
   "source": [
    "##### Running Random Minibatch Selection (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9462132",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.clear_param_store()\n",
    "random_optimizer = Adam({\"lr\": LEARNING_RATE})\n",
    "random_losses, random_overall_accuracies, random_rare_accuracies = train_with_random_minibatches(\n",
    "    model, guide, active_dataset, random_optimizer, NUM_EPOCHS, \n",
    "    batch_size=ACTIVE_BATCH_SIZE,\n",
    "    class_names=class_names\n",
    ")\n",
    "\n",
    "# Save random model\n",
    "random_model_params = {name: param.data.clone() for name, param in pyro.get_param_store().items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac456622",
   "metadata": {},
   "source": [
    "##### Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4e214f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_comparison(\n",
    "    active_losses, random_losses,\n",
    "    active_overall_accuracies, random_overall_accuracies,\n",
    "    active_rare_accuracies, random_rare_accuracies\n",
    ")\n",
    "\n",
    "# Restore active model for visualization\n",
    "pyro.clear_param_store()\n",
    "for name, param in active_model_params.items():\n",
    "    pyro.param(name, param)\n",
    "\n",
    "# Compute final uncertainties\n",
    "all_data = torch.stack([active_dataset[i][0] for i in range(len(active_dataset))]).to(device)\n",
    "uncertainties = compute_bayesian_uncertainty(\n",
    "    model, guide, all_data, num_samples=10, uncertainty_type='entropy'\n",
    ")\n",
    "active_dataset.update_uncertainties(uncertainties.cpu())\n",
    "\n",
    "# Analyze dataset usage patterns\n",
    "analyze_dataset_usage(active_dataset, class_names)\n",
    "\n",
    "# Visualize most uncertain samples\n",
    "visualize_uncertain_samples(active_dataset, k=10, class_names=class_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML703",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
